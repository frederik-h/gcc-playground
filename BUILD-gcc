#!/bin/bash

set -e -x

mkdir -p "$T"/build-gcc
cd "$T"/build-gcc/

rm -f .have-build .have-install
if ! test -f .have-configure; then
  (# --with-cuda-driver-lib is specified not for the CUDA Driver Library (which
  # is found in default search paths) but for the CUDA Runtime Library as used
  # in a few tests in the libgomp testsuite.  This is less intrusive than
  # setting LIBRARY_PATH and LD_LIBRARY_PATH environment variables, or adding
  # support for --with-cuda-runtime-lib.
  "$T"/source-gcc/configure \
    --prefix= \
    --enable-languages=all \
    --enable-werror \
    --enable-checking=yes \
    --enable-offload-targets=nvptx-none="$T"/install/offload-nvptx-none,x86_64-intelmicemul-linux-gnu="$T"/install/offload-x86_64-intelmicemul-linux-gnu,hsa \
    --with-cuda-driver-include=$CUDA/targets/x86_64-linux/include \
    --with-cuda-driver-lib=$CUDA/targets/x86_64-linux/lib\ -Wl,-rpath,$CUDA/targets/x86_64-linux/lib \
    --with-hsa-runtime-include=$HSA_RUNTIME/inc \
    --with-hsa-runtime-lib=$HSA_RUNTIME \
    CC=gcc-4.6 \
    CXX=g++-4.6 \
    $CONFIGURE_ &&
  touch .have-configure) 2>&1 | tee -a log_build
fi
test -f .have-configure
(make "$@" &&
touch .have-build) 2>&1 | tee -a log_build
test -f .have-build
